{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from  scipy.ndimage import zoom as imzoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from SpatialTransformer2D import SpatialTransformer2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ConvST_net = SpatialTransformer2d( num_input_channels = 1,\n",
    "                 feature_net = None,\n",
    "                 out_patch_size = 32,\n",
    "                 out_stride = 32,\n",
    "                 min_zoom = 1.0,\n",
    "                 max_zoom = 1.0,\n",
    "                 min_tilt = 1.0,\n",
    "                 max_tilt = 1.0,\n",
    "                 max_rot = 0.0,\n",
    "                 max_shift = 0.0,\n",
    "                 mrSize = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = Image.open('/home/old-ufo/Dropbox/Mik/graf/img1.png').convert('RGB')\n",
    "img = np.sum(np.array(img)/255.0, axis = 2)\n",
    "#img = imzoom(img, (0.5,0.5))\n",
    "var_image = torch.autograd.Variable(torch.from_numpy(img.astype(np.float32)))\n",
    "var_image_reshape = var_image.view(1, 1, var_image.size(0),var_image.size(1))\n",
    "#var_image_reshape = var_image_reshape.cuda()\n",
    "#ConvST_net.cuda()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img2 = Image.open('/home/old-ufo/Dropbox/Mik/graf/img2.png').convert('RGB')\n",
    "img2 = np.sum(np.array(img2)/255.0, axis = 2)\n",
    "#img = imzoom(img, (0.5,0.5))\n",
    "var_image2 = torch.autograd.Variable(torch.from_numpy(img2.astype(np.float32)))\n",
    "var_image_reshape2 = var_image2.view(1, 1, var_image2.size(0),var_image2.size(1))\n",
    "#var_image_reshape2 = var_image_reshape2.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_image,aff_pts = ConvST_net(var_image_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_image2,aff_pts2 = ConvST_net(var_image_reshape2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-5181.1489\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = torch.sum(out_image - out_image2)\n",
    "\n",
    "print loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HardNet import HardNet\n",
    "hardnet = HardNet()\n",
    "checkpoint = torch.load('HardNetLib.pth')\n",
    "hardnet.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SparseImgRepresenter(nn.Module):\n",
    "    def __init__(self, \n",
    "             detector_net = None,\n",
    "             descriptor_net = None,    \n",
    "             use_cuda = False):\n",
    "        super(SparseImgRepresenter, self).__init__()\n",
    "        self.detector = detector_net;\n",
    "        self.descriptor = descriptor_net;\n",
    "        return\n",
    "    def forward(self, input_img):\n",
    "        aff_norm_patches, LAFs = self.detector(input_img)\n",
    "        descs = self.descriptor(aff_norm_patches);\n",
    "        return aff_norm_patches, LAFs, descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SIRNet = SparseImgRepresenter(detector_net = ConvST_net, descriptor_net = hardnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aff_norm_patches, LAFs, descs = SIRNet(var_image_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance_matrix_vector(anchor, positive):\n",
    "    \"\"\"Given batch of anchor descriptors and positive descriptors calculate distance matrix\"\"\"\n",
    "\n",
    "    d1_sq = torch.sum(anchor * anchor, dim=1)\n",
    "    d2_sq = torch.sum(positive * positive, dim=1)\n",
    "    eps = 1e-6\n",
    "    return torch.sqrt((d1_sq.expand(anchor.size(0), anchor.size(0)) + torch.t(d2_sq.expand(positive.size(0), positive.size(0)))\n",
    "                      - 2.0 * torch.bmm(positive.unsqueeze(0), torch.t(anchor).unsqueeze(0)).squeeze(0))+eps)\n",
    "\n",
    "def LAFs_to_H_frames(aff_pts):\n",
    "    H3_x = torch.Tensor([0, 0, 1 ]).unsqueeze(0).unsqueeze(0).expand_as(aff_pts[:,0:1,:]);\n",
    "    H3_x = torch.autograd.Variable(H3_x)\n",
    "    return torch.cat([aff_pts, H3_x], dim = 1)\n",
    "def get_GT_correspondence_indexes(aff_pts1,aff_pts2, H1to2, dist_threshold = 4):\n",
    "    LHF2 = LAFs_to_H_frames(aff_pts2)\n",
    "    LHF2_reprojected_to_1 = torch.bmm(H1to2.unsqueeze(0).expand_as(LHF2), LHF2);\n",
    "    LHF2_reprojected_to_1 = LHF2_reprojected_to_1 / LHF2_reprojected_to_1[:,2:,2:].expand_as(LHF2_reprojected_to_1);\n",
    "    just_centers1 = aff_pts1[:,:,2];\n",
    "    just_centers2_repr_to_1 = LHF2_reprojected_to_1[:,0:2,2];\n",
    "    dist  = distance_matrix_vector(just_centers1, just_centers2_repr_to_1)\n",
    "    min_dist, idxs_in_2 = torch.min(dist,1)\n",
    "    plain_indxs_in1 = torch.autograd.Variable(torch.arange(0, idxs_in_2.size(0))).cuda()\n",
    "    mask =  min_dist <= dist_threshold\n",
    "    return min_dist[mask], plain_indxs_in1[mask], idxs_in_2[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-3.1480e-02 -1.1687e-01  7.4637e-02  ...  -8.0522e-02  5.3022e-02  1.5812e-02\n",
       "-6.3397e-02 -1.1175e-01  2.4787e-01  ...  -4.0955e-02  1.8381e-02  5.1095e-02\n",
       " 1.0474e-02 -9.5838e-02  1.0675e-01  ...  -1.1603e-01  1.0846e-01 -4.7470e-02\n",
       "                ...                   â‹±                   ...                \n",
       " 8.6725e-02 -2.1020e-02  7.7554e-02  ...   7.2322e-02 -9.1421e-02  1.0271e-01\n",
       " 8.2311e-02  4.6112e-03  5.9497e-02  ...   2.4793e-02  4.3884e-02 -1.2280e-01\n",
       "-9.8883e-02 -6.6458e-02  7.0221e-02  ...  -2.0086e-02  1.6380e-03  5.9372e-02\n",
       "[torch.FloatTensor of size 500x128]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists, idxs_in1, idxs_in2 = get_GT_correspondence_indexes(aff_pts, aff_matr_2, H1to2_v, dist_threshold = 5);\n",
    "#print gt_idxs\n",
    "def train_SIR_Net_in_UCN_style(img1,img2, H1to2p, SIRNet):\n",
    "    \n",
    "    return\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
